# Claude Development Log - Performance Evaluation Analysis System

## Project Overview
System for extracting and analyzing year-end performance evaluations from webarchive files using LLM analysis.

**Created**: 2025-11-26
**Status**: ✅ Complete and tested
**Purpose**: Process employee performance evaluations for committee review

---

## System Components

### 1. Data Extraction Layer

**File**: `extract_evaluations.py`
**Purpose**: Extract structured data from macOS webarchive evaluation files
**Key Features**:
- Parses binary plist webarchive format
- Extracts HTML content using BeautifulSoup
- Identifies and categorizes evaluators (self/peer/client)
- Extracts 4 competency categories with questions, ratings, and notes
- Outputs clean JSON for LLM consumption

**Key Functions**:
- `extract_html_from_webarchive()` - Converts webarchive to HTML
- `extract_employee_info()` - Gets employee name, role, averages
- `extract_questions()` - Parses questions with star ratings and notes
- `determine_evaluator_type()` - Auto-detects self/peer/client by role
- `extract_competency_data()` - Extracts all 4 competency categories
- `extract_strengths_weaknesses()` - Gets strength/weakness lists
- `process_directory()` - Batch processes multiple employees

**Dependencies**: `beautifulsoup4`, Python 3.7+

**Recent Changes** (2025-11-26):
- Added automatic client detection based on evaluator role containing "Client"
- Updated `determine_evaluator_type()` to accept `evaluator_role` parameter
- Tested and verified with Alan Reskin data (9 evaluations, correct categorization)

### 2. LLM Analysis Layer

**File**: `llm_prompt_template.md`
**Purpose**: Comprehensive prompt templates for performance review analysis
**Prompts Included**:

1. **System Prompt** - Sets context for HR analysis role
2. **Main Analysis Prompt** - Full year-end review (9 sections)
3. **Follow-up Question Prompt** - Specific employee inquiries
4. **Comparative Analysis Prompt** - Multi-employee comparison
5. **Quick Insights Prompt** - 200-word committee briefs
6. **Outlier Investigation Prompt** - Deep dive unusual ratings
7. **Client Feedback Analysis Prompt** - Client-specific insights
8. **Development Plan Prompt** - Actionable improvement plans

**Output Structure**:
- Executive Summary
- Quantitative Analysis (scores by competency/evaluator type)
- Qualitative Analysis (strengths/development areas)
- Competency Deep Dive (TECH EXCELLENCE, TEAMWORK, CUSTOMER CHAMPION, GROWTH MINDSET)
- Evaluator Comparison (self vs peer vs client)
- Pattern Analysis (frequency themes)
- Notable Observations (highlights/concerns)
- Contextual Insights (role appropriateness)
- Committee Summary (1-page brief)

### 3. Documentation Layer

**Files**:
- `README.md` - Complete system documentation
- `QUICKSTART.md` - 5-minute quick start guide
- `Evaluation_Structure_Analysis.md` - HTML structure reference
- `.claude.md` - This development log

---

## Data Structure

### Input: Webarchive Files
- Format: macOS webarchive (binary plist)
- Content: HTML with employee evaluations
- Source: Performance evaluation platform

### Output: JSON Structure
```json
{
  "employee": {
    "name": "string",
    "role": "string",
    "total_average": float,
    "evaluations_received": int,
    "evaluations_given": int
  },
  "evaluations": [
    {
      "evaluator_name": "string",
      "evaluator_role": "string",
      "evaluator_type": "self|peer|client",
      "evaluation_average": float,
      "competencies": {
        "TECH EXCELLENCE": [...],
        "TEAMWORK": [...],
        "CUSTOMER CHAMPION": [...],
        "GROWTH MINDSET": [...]
      },
      "strengths": ["string", ...],
      "weaknesses": ["string", ...],
      "additional_thoughts": "string"
    }
  ]
}
```

### Question Structure
```json
{
  "question": "string (Spanish)",
  "stars": int (0-4),
  "level_description": "string (Spanish)",
  "note": "string (optional context from evaluator)"
}
```

---

## Evaluation System Details

### Rating Scale
- **4 stars**: Exceptional - Exceeds expectations significantly
- **3 stars**: Strong - Consistently meets and often exceeds
- **2 stars**: Developing - Meets some expectations, needs improvement
- **1 star**: Below expectations - Significant development needed
- **0 stars**: Not demonstrated

### Competency Categories

1. **TECH EXCELLENCE**
   - Technical understanding and stack knowledge
   - Consistency in achieving objectives
   - Autonomy in technical decisions

2. **TEAMWORK**
   - English proficiency
   - Active collaboration
   - Communication effectiveness
   - Conflict resolution

3. **CUSTOMER CHAMPION**
   - Product/business knowledge integration
   - Task/team tracking effectiveness

4. **GROWTH MINDSET**
   - Stepping out of comfort zone
   - Feedback consistency
   - Knowledge sharing

### Evaluator Types
- **self**: Employee's self-evaluation
- **peer**: Internal colleague/team member
- **client**: External client (detected by "Client" in role field)

---

## Testing Results

### Test Case: Alan Reskin (QA Senior)
**Date**: 2025-11-26
**Status**: ✅ PASSED

**Input**: `Alan Reskin.webarchive`
**Output**: `alan_reskin_data.json`

**Results**:
- ✅ Extracted 9 evaluations correctly
- ✅ Categorized evaluators: 1 self, 5 peer, 3 client
- ✅ All competencies extracted (4 categories per evaluation)
- ✅ Star ratings captured accurately
- ✅ Notes extracted where present
- ✅ Strengths/weaknesses lists complete
- ✅ Averages calculated correctly

**Score Breakdown**:
- Total Average: 3.3/4.0
- Self Average: 2.81
- Peer Average: 3.11
- Client Average: 3.83

**Key Observation**: Significant gap between self (2.81) and client (3.83) ratings suggests under-self-assessment.

**Evaluators Identified**:
```
Self:
  - Alan Reskin (QA Senior) - 2.81

Peers:
  - Alberto Cols (SD Senior) - 3.52
  - Gaspar Tovar (SD Senior) - 3.21
  - Juan Casilla (SD Senior) - 3.10
  - Vidya Shankar Sarvepalli (QA Senior) - 2.73
  - Wenceslao Hoz (SD Senior) - 3.00

Clients:
  - Francisco Hurtado (Client) - 4.00
  - Rakesh Radhakrishnan (Client) - 4.00
  - Robin Hendrix (Client) - 3.50
```

---

## Setup Instructions

### First-Time Setup
```bash
cd "/Volumes/T7/Users/Shared/Mahisoft GD/Admin - Teams & Others/YEAH/2025"
python3 -m venv venv
source venv/bin/activate
pip install beautifulsoup4
```

### Usage
```bash
# Activate environment
source venv/bin/activate

# Process single file
python extract_evaluations.py "Employee Name.webarchive" "output.json"

# Process all files in directory
python extract_evaluations.py . json_output/
```

### Virtual Environment
- **Location**: `./venv/`
- **Python Version**: System Python 3
- **Dependencies**: beautifulsoup4, soupsieve, typing-extensions

---

## Workflow

### Standard Committee Preparation Workflow

1. **Extract Data**
   ```bash
   source venv/bin/activate
   python extract_evaluations.py . json_output/
   ```

2. **Analyze Individuals**
   - Read each employee's JSON file
   - Use main analysis prompt from `llm_prompt_template.md`
   - Save analysis outputs

3. **Generate Quick Briefs**
   - Use quick insights prompt for 200-word summaries
   - Suitable for rapid committee review

4. **Comparative Analysis**
   - Compare employees in similar roles
   - Identify outliers and patterns

5. **Prepare Follow-ups**
   - Anticipate committee questions
   - Have detailed data ready

6. **Committee Presentation**
   - Present summaries
   - Deep dive as needed
   - Reference specific data points

---

## Known Issues & Solutions

### Issue: No module named 'bs4'
**Solution**: Activate virtual environment first
```bash
source venv/bin/activate
```

### Issue: Client evaluators showing as "peer"
**Status**: ✅ FIXED (2025-11-26)
**Solution**: Updated `determine_evaluator_type()` to check role for "Client" keyword

### Issue: Webarchive extraction fails
**Possible Causes**:
- File not fully loaded before saving
- Browser didn't expand all evaluation panels
**Solution**: Open in Safari, expand all panels, re-save

### Issue: Missing notes
**Cause**: Notes are in HTML-encoded popovers
**Status**: ✅ Handled - Notes extracted from `data-content` attribute

---

## File Inventory

### Core System Files
- ✅ `extract_evaluations.py` - Data extraction script (350+ lines)
- ✅ `llm_prompt_template.md` - LLM prompt templates
- ✅ `README.md` - Complete documentation
- ✅ `QUICKSTART.md` - Quick start guide
- ✅ `Evaluation_Structure_Analysis.md` - HTML structure docs
- ✅ `.claude.md` - This file

### Environment
- ✅ `venv/` - Python virtual environment (created)
- ✅ `.gitignore` - Should exclude venv/, json_output/, *.webarchive

### Test Data
- ✅ `Alan Reskin.webarchive` - Test input file
- ✅ `Alan Reskin.html` - Original HTML (reference)
- ✅ `alan_reskin_data.json` - Test output (9 evaluations)

### Output Directories
- ✅ `json_output/` - Extracted JSON files (created during testing)

---

## Future Enhancements

### Potential Improvements
1. **Client Detection Enhancement**
   - Maintain configurable list of known client names
   - Add regex patterns for client email domains
   - Allow manual override via config file

2. **Data Validation**
   - Verify all questions have ratings
   - Check for missing competencies
   - Flag incomplete evaluations

3. **Batch Analysis**
   - Python script to call LLM API for all employees
   - Automated report generation
   - CSV export for dashboards

4. **Visualization**
   - Generate charts for competency comparisons
   - Radar plots for individual competencies
   - Heatmaps for team comparisons

5. **Integration**
   - Export to Excel with formulas
   - Import to HRIS systems
   - Slack/email notifications

6. **Multi-language Support**
   - Translate prompts for English-only evaluators
   - Handle mixed-language notes

### Not Implemented (By Design)
- Time estimates for tasks (per user requirements)
- Automatic promotion recommendations (human decision)
- Performance improvement plan generation (sensitive HR matter)

---

## Usage Patterns

### Pattern 1: Individual Deep Dive
```
User: Read json_output/Alan_Reskin.json
[Copy main analysis prompt with JSON]
User: What are the main concerns?
User: How does his self-assessment compare to others?
```

### Pattern 2: Team Comparison
```
User: Read json_output/Employee1.json, Employee2.json, Employee3.json
[Use comparative analysis prompt]
User: Who is ready for promotion?
User: Focus on client feedback for Employee2
```

### Pattern 3: Committee Preparation
```
[Generate quick briefs for all employees]
[Flag outliers]
[Prepare FAQs for each employee]
[Have detailed data ready for questions]
```

---

## Code Maintenance Notes

### Critical Functions (Don't Break)
- `determine_evaluator_type()` - Client detection logic
- `count_stars()` - Rating extraction
- `extract_note()` - HTML-encoded note parsing
- `extract_questions()` - Question/rating pairing logic

### Safe to Customize
- Question text patterns (if platform changes)
- Output JSON structure (add fields as needed)
- Client detection keywords
- Strength/weakness extraction

### Extension Points
- Add new evaluator types beyond self/peer/client
- Add competency categories if platform adds them
- Custom aggregation functions
- Export formats (CSV, Excel, etc.)

---

## Version History

### v1.0 - 2025-11-26 (Initial Release)
- ✅ Data extraction from webarchive files
- ✅ Automatic evaluator type detection
- ✅ JSON output with full evaluation data
- ✅ Comprehensive LLM prompt templates
- ✅ Complete documentation suite
- ✅ Tested with real data (Alan Reskin - 9 evaluations)
- ✅ Client detection working correctly

---

## Context for Future Sessions

### What This System Does
Processes employee performance evaluations exported as webarchive files from a web platform. Extracts structured data (JSON) and provides LLM prompts to analyze the data for year-end committee reviews.

### Why It Was Built This Way
- **Webarchive format**: User saves evaluations from web browser
- **JSON intermediate**: Clean data for LLM, portable, debuggable
- **Separate extraction/analysis**: Extraction is deterministic, analysis is flexible
- **Multiple prompts**: Different committee needs (quick brief vs deep dive)

### What's Already Working
- Full extraction pipeline
- Automatic client/peer/self categorization
- All 4 competencies captured
- Notes, strengths, weaknesses extracted
- Tested and verified with real data

### What User Will Do Next
1. Place more employee webarchive files in this directory
2. Run extraction script on all files
3. Use LLM prompts to analyze each employee
4. Ask follow-up questions as needed (in this session)
5. Present findings to committee

### Key Files User Will Interact With
- Input: `*.webarchive` files (from their evaluation platform)
- Script: `extract_evaluations.py` (run via command line)
- Output: `json_output/*.json` (read into LLM sessions)
- Prompts: `llm_prompt_template.md` (copy/paste for analysis)

### Important Reminders
- User wants observations, NOT recommendations
- Compare self vs peer vs client evaluations
- Flag outliers (>0.5 point gaps)
- Keep analysis data-driven and specific
- User will ask follow-up questions - maintain context

---

## Quick Command Reference

```bash
# Setup (first time)
python3 -m venv venv
source venv/bin/activate
pip install beautifulsoup4

# Daily usage
source venv/bin/activate
python extract_evaluations.py . json_output/

# Verify extraction
ls json_output/

# Check extracted data
python3 -c "import json; data=json.load(open('json_output/Employee.json')); print(data['employee'])"

# Count evaluations by type
python3 -c "import json; data=json.load(open('json_output/Employee.json')); print(f\"Self: {sum(1 for e in data['evaluations'] if e['evaluator_type']=='self')}, Peer: {sum(1 for e in data['evaluations'] if e['evaluator_type']=='peer')}, Client: {sum(1 for e in data['evaluations'] if e['evaluator_type']=='client')}\")"
```

---

**Last Updated**: 2025-11-26
**Next Review**: When processing additional employees or adding new features
**Maintained By**: Claude Code sessions in this directory
